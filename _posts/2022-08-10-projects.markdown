## ML

**[poker-player](https://github.com/samuelarnesen/poker-player)** -- tried to teach myself the basics of graph neural networks by coding up a GNN-based classifier of Texas Hold'em hands and then tried to use that as the foundation of a poker playing AI agent. The classifier worked well but the playing mechanism very much did not.

**[connect-four](https://github.com/samuelarnesen/board-games/tree/master/connect_four)** -- agent learns to play connect four by playing against itself from random starting positions. Can beat me every time but I've never tried it against someone who's any good. Also includes a nifty front-end built by Rohit Dilip

**[spinning-up-rl](https://github.com/samuelarnesen/learning-rl)** -- built models from the OpenAI Spinning Up RL paper series. Completed the main papers double DQNs, simple policy gradients, deep deterministic policy gradients, and intrinsic curiosity modules, among others.

**[gpu-practice](https://colab.research.google.com/drive/1HZi076im-6v35Y6XBqXpJQ6SN1Yay9Za#scrollTo=b64d4dbb)** -- taught myself the basics of Numba/GPU programming by completing a series of puzzles made by Prof. Sasha Rush. Technically this isn't ML but it fits this category the best.

**[maximum-entropy-elections](https://github.com/samuelarnesen/into-the-wild)** -- inspired by a paper on using maximum entropy models to capture influence on the Supreme Court, we attempted to extend this to Senate voting records. We were able to replicate the Supreme Court analysis but the method 100% did not work for the Senate (you can't really capture influence networks when most members vote the same way almost every time). Done jointly with Rohit Dilip.

**[thesis](https://github.com/samuelarnesen/walkthrough-agent)** -- My undergraduate thesis was on whether we could finetune a language model to play text-based games such as Zork. Potentially, this could've been a toy case for the broader question of whether large language models could be improved via RL in the wild. I spent a really long time creating a dataset to train the agents but even then I think it was too small to get the agents to do much of interest (the pre-trained large language models did actually improve by training on other text-based games but my personal take is that was primarily because it did a better job of learning the relevant vocabulary). That said, I did learn an enormous amount about RL, large language models, and CS research in the process. Completed under advisor Prof. Karthik Narasimhan.

## Data Analysis

**[thesis-gender](https://github.com/samuelarnesen/thesis_gender)** -- got a bunch of data on people who submitted theses at Princeton and looked into whether the use of collective or individual pronouns varied by the assumed gender of the author (e.g. "We show" vs "I show"). It turned out that women were much more likely to use individual pronouns, but this effect was entirely due to differences in major selection by gender. STEM majors were much more likely to use collective pronouns while social science and humanities majors used individual pronouns more frequently. As the STEM majors had many more men than women, men ended up using collective pronouns more often overall. Within each major though, there was zero difference between genders.

**[turkey-trot](https://github.com/samuelarnesen/turkey_trot)** -- in 2019, I ran a large 10K race on Thanksgiving and it really bothered me that despite accurately selecting my pace group, I spent most of the race weaving in and out of people that I had to pass to maintain my (slow) pace. So I downloaded the results from online and did some simple analysis. It turns out that people are horrible at selecting their pace group -- with the exception of the people in the sub-6:30 mile groups, the average racetime was basically the same across all pace groups. Both the 9 minute pace group (my group) and the 7:30 group, on average, ran at the same speed! (Incidentally, that speed averaged out to just under 10 minutes/mile for both groups, so no one was actually right).

**[donor-directions](https://github.com/samuelarnesen/donor-directions)** -- We downloaded the donor files to all the major candidates in the 2020 primary elections. We then manually mapped a couple thousand job descriptions to their associated industry and used that to train a very simple classifier. We then did some simple analyses on the results (mostly just normal plots and simple clustering mechanisms). We did a longer write-up of the results (here)while we were bored during the earlier pandemic. Done jointly with Rohit Dilip.

**[nba-draft](https://github.com/samuelarnesen/nba-draft)** -- If an ESPN Big Board says Player A is better than Player B, but B is drafted ahead of A, which player is likely to turn out better? Suprisingly, the ESPN Big Board was actually correct 2/3rds of the time. Obviously there's a lot of caveats here (e.g. higher draft picks are more likely to get greater playing time and the dataset I created only had 10 years of big boards) but that still went against what I would have guessed. 

## Games

**[nba-gm](https://github.com/samuelarnesen/nba-gm)** -- during peak pandemic boredom, I created a fantasy-basketball-like game that I could play against friends. It takes the 5 best years of every NBA all-star post-1980 and each season (you play for 10 seasons), a subset of them become available for drafting. We added in some twists to improve game play - one was that the players will only play for a varying number of seasons (do you want 3 seasons of Michael Jordan or 5 seasons of Tim Duncan?) while another was that only some players would have their names revealed during the draft, with the remainder showing just their stats from a small sample of games (do you want to take a guaranteed good-but-not-incredible Alex English or an unknown guy who looks like Magic Johnson during a 5 game sample?). We ended up playing this a bunch and it was really fun, although the terminal-based UI was terrible. The data was scraped by Will Arnesen and the game concept was come up with by Matt Feng. 

**[avalon](https://github.com/samuelarnesen/board-games/tree/master/avalon)** -- we played about two hundred games of Avalon during the pandemic but simple stats didn't do a great job at capturing who was the best since the bad guys win most of the time. So I constructed a bunch of different "advanced" stats (some of which were pretty reasonable while others very much were not) to figure out which one of us was actually the most skilled.

**[boggle](https://github.com/samuelarnesen/board-games/tree/master/boggle)** -- I played a lot of boggle early in the pandemic with my siblings and we wanted to figure out which words we missed after the game was over. This is easily available online but it was easy enough to create so figured I'd make my own

**[bull-market](https://github.com/samuelarnesen/board-games/tree/master/contract_game)** -- my brother figured that an auction would be a neat gameplay mechanism for a board game. So he created a game where the US was divided up into 100ish small territories that each produced a variety of resources, with the precise values based off of actual DOE/DOA production data. Each turn, a contract would come up that would represent a demand function for a particular good in a particular city and would specify the auction mechanism it would use (second-price auction and Cornot were the two options). Each player would then submit their bid, with their cost curve being based on the territories they owned and the distance from the city demanding the resource. It took awhile to compute the production costs each turn so I coded the game to speed things up. Unfortunately, the game ended up with a runaway-winner problem but it was still pretty fun to create and mess around with.

**[mastermind](https://github.com/samuelarnesen/board-games/tree/master/mastermind)** --  my siblings and I played mastermind one morning and we wanted to figure out the minimum number of guesses we needed to solve each puzzle so I coded up this very simple solver

**[spoons](https://github.com/samuelarnesen/board-games/tree/master/spoons)** -- I had a disagreement with my brother about whether the player in the first position has an advantage in the card game Spoons. I thought they had an advantage because they can see more cards while he thought they had a disadvantage because there are a greater number of potentially-useful cards that they'll never have access to. So I coded up a simple simulator to determine who was right. Unfortunately, it was inconclusive because the result is very sensitive to differences in the input -- if a regular player can draw from their pile at the same speed as the first player can draw from the deck, then there is a disadvantage to going first. However, if the first player can draw faster, that disadvantage shrinks. At a high enough speed difference, the first player has an advantage. 

**[spycraft](https://github.com/samuelarnesen/board-games/tree/master/spycraft)** -- Spycraft is a pretty fun social deception party game that you can buy online. We wanted to play it in person with my extended family and to create our own cards so I coded up an extremely-simple version of it. 

**[bgfw](https://github.com/samuelarnesen/board-games/tree/master/bgfw)** -- A relatively simple programming framework for building terminal-based board games. I used it to create the auction-based board game.
